<!DOCTYPE html>
<html>
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-108813396-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-108813396-1');
    </script>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../static/css/style.css">
    <link href="../static/css/code.css" rel="stylesheet" type="text/css">
    <title>Univariate k-Nearest Comparison (Trustworthy Models) - Samuel Taylor</title>
  </head>
  <body>
<span class="breadcrumb"><a href="../index.html">Home</a> &gt; <a href="index.html">Articles</a> &gt; Univariate k-Nearest Comparison (Trustworthy Models)</span>
<h1>Univariate k-Nearest Comparison (Trustworthy Models)</h1>
<p>We often fly blind in the world of machine learning. Our model outputs an estimate for revenue from clicking on a certain ad, or the amount of time until a new edition of a book comes out, or how long it will take to drive a certain route. Typically these estimates are in the form of a single number with implicitly high confidence. Trusting these models can be foolish! Perhaps our models are high-tech con men -- Frank Abagnale reborn in the form of a deep neural net.</p>
<p>If we want to call ourselves "Data Scientists", perhaps it is time to behave
like scientists do in other fields.</p>
<p>I didn't study a natural science (like astronomy or biology), but I did take a
few physics classes with labs. One such lab required us to find the
gravitational constant by dropping a metal ball from a variety of heights.
During the experiment we were careful to record uncertainties in our
measurements, and we propagated uncertainty through to our final estimate of the
gravitational constant. If I had turned in a lab report claiming g = 9.12 m/s^2
(without any uncertainty estimate), I would have lost points.</p>
<p>Good scientific measurements come with uncertainty. A ruler or measuring tape is
only so precise. When it comes to machine learning, though, this focus on
uncertainty disappears.</p>
<p>For example, see this common formulation of the learning problem from the book
<em>Learning from Data</em> <a href="#fn0">[0]</a> (no shade -- I love this book):</p>
<blockquote>
<p>There is a target to be learned. It is unknown to us. We have examples
generated by the target. The learning algorithm uses these examples to look
for a hypothesis that approximates the target.</p>
</blockquote>
<p>This unknown target function they call <em>f</em>: <em>X</em> -&gt; <em>Y</em> (where <em>X</em> is the input/feature
space and <em>Y</em> is the output/target space). The hypothesis approximating the
target they denote as <em>g</em>: <em>X</em> -&gt; <em>Y</em> (and, if our learning algorithm is successful,
we can say <em>g ≈ f</em>).</p>
<p>In the case of regression, the output of our learning algorithm is a function
which produces a continuous-valued output. But this output is a point estimate.
It has no sense of uncertainty <a href="#fn1">[1]</a>!</p>
<p>Others have of course noted the importance of uncertainty estimation before me.
One such person is José Hernández-Orallo (a professor at Polytechnic University
of Valencia), whose paper <a rel="nofollow" href="https://dl.acm.org/doi/abs/10.1145/2641758"><em>Probabilistic reframing for cost-sensitive
regression</em></a> I found while
researching <a href="/articles/how-to-handle-class-imbalance.html">class
imbalance</a>. I would
be misrepresenting his work to claim this paper is solely about
uncertainty/reliability estimation, but he describes some neat ideas
worth exploring.</p>
<p>Rather than finding a function <em>g</em>: <em>X</em> -&gt; <em>Y</em> approximating the true underlying
function <em>f</em>, we could instead seek to find a probability density function <em>h(y |
x)</em>. In other words, a function to which we can still pass some features (<em>x</em>) but
one describing a distribution instead of a point estimate. Because we are
estimating a probability density function conditioned on the input features,
this idea is called conditional density estimation.</p>
<p>To hear Hernández-Orallo tell it, many methods for conditional density
estimation are suboptimal. The mean of the distributions they output is
typically worse than a point estimate would have been. They are often slow. And
in many cases, the distributions don't end up being multi-modal anyway. Thus,
the paper asserts we can get by with a method to provide a
normal (Gaussian) density function for most cases.</p>
<p>Normal distributions are parametrized by a mean and a standard deviation. Taking
a point estimate (from any regression model) as the mean, we still need to
determine the standard deviation. The paper describes a few different approaches
for doing this (and is worth reading if you have the time). For the sake of this
post, I'll focus on a technique called "univariate k-nearest comparison". A
simple Python implementation follows:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">univariate_knearest_comparison</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">test_point</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;Weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_point</span><span class="p">)</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="p">((</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span> <span class="k">for</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span> <span class="ow">in</span> <span class="n">Q</span><span class="p">),</span>
        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">prediction</span><span class="p">),</span>
    <span class="p">)[:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">variance_estimate</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span>
        <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">y_true</span> <span class="ow">in</span> <span class="n">neighbors</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">variance_estimate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

<p>Hernández-Orallo describes this procedure as looking "for the closest
estimations in the training set to the estimation for example <em>x</em>", then comparing
"their true values with the estimation for <em>x</em>".</p>
<p>This technique is cool because it can be applied to any regression model. By
using the training set (or a validation set) in this clever way, we can enrich
any model with the ability to estimate uncertainty (thus gaining the second
aspect of what we've <a href="/articles/trustworthy-models.html">been calling trustworthy
models</a>).</p>
<p>If you've not read previous posts in this series, we've been working with a
dataset about fish. From their dimensions, we're trying to predict their weight.
It's totally a toy/unrealistic problem, but it's pedagogically useful.</p>
<p>We'll start by training a linear model.</p>
<div class="highlight"><pre><span></span><span class="n">ct</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;Length1&#39;</span><span class="p">,</span> <span class="s1">&#39;Length2&#39;</span><span class="p">,</span> <span class="s1">&#39;Length3&#39;</span><span class="p">,</span> <span class="s1">&#39;Height&#39;</span><span class="p">,</span> <span class="s1">&#39;Width&#39;</span><span class="p">]),</span>
    <span class="p">(</span><span class="s1">&#39;ohe&#39;</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]),</span>
<span class="p">])</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">ct</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">fish</span><span class="p">,</span> <span class="n">fish</span><span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">])</span>
</pre></div>

<p>Then, we'll run the univariate k-nearest comparison function from above.</p>
<div class="highlight"><pre><span></span><span class="n">new_fish</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;Species&quot;</span><span class="p">:</span> <span class="s2">&quot;Bream&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Weight&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
            <span class="s2">&quot;Length1&quot;</span><span class="p">:</span> <span class="mf">31.3</span><span class="p">,</span>
            <span class="s2">&quot;Length2&quot;</span><span class="p">:</span> <span class="mi">34</span><span class="p">,</span>
            <span class="s2">&quot;Length3&quot;</span><span class="p">:</span> <span class="mf">39.5</span><span class="p">,</span>
            <span class="s2">&quot;Height&quot;</span><span class="p">:</span> <span class="mf">15.1285</span><span class="p">,</span>
            <span class="s2">&quot;Width&quot;</span><span class="p">:</span> <span class="mf">5.5695</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">pred</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">univariate_knearest_comparison</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">fish</span><span class="p">,</span> <span class="n">new_fish</span><span class="p">)</span>
<span class="c1"># (646.1153309725989, 4896.726887004621)</span>
</pre></div>

<p>With our prediction and variance estimate in hand, we can draw a normal
distribution.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">st</span>

<span class="k">def</span> <span class="nf">plot_normal</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;coral&#39;</span><span class="p">,</span> <span class="n">do_lims</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">width</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">)</span> <span class="o">*</span> <span class="mf">3.8</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">-</span> <span class="n">width</span><span class="p">,</span> <span class="n">pred</span> <span class="o">+</span> <span class="n">width</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">))</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">do_lims</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">)</span>

        <span class="n">ylo</span><span class="p">,</span> <span class="n">yhi</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">yhi</span><span class="p">)</span>

        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Conditional density of fish weight given features&#39;</span><span class="p">)</span>

<span class="n">plot_normal</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>
</pre></div>

<p><img src="/static/img/uknc0.png"></p>
<p>Here's a little graph with conditional density estimates for several different
fish on it.</p>
<p><img src="/static/img/uknc1.png"></p>
<p>This is a strong step in the direction of being more scientific in our modeling
efforts. We've examined a few methods for uncertainty estimation in this series,
and we'll evaluate the quality of these techniques at a later date.</p>
<p><em>If you found this interesting, consider <a rel="nofollow" href="https://twitter.com/SamuelDataT">following me on Twitter</a>.</em></p>
<hr />
<h3>Footnotes</h3>
<ol start="0">
<li id="fn0">Abu-Mostafa, Y. S., Magdon-Ismail, M., &amp; Lin, H. (2012).
_Learning from data: A short course_. United States: AMLBook.com.</li>
<li id="fn1">I recognize that I'm equivocating on the word "uncertainty" to some extent.
Still, I think this is a useful idea even if only as an analogy.</li>
</ol>
  </body>
</html>